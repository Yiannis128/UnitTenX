# Copyright 2025 Claudionor N. Coelho Jr

import json
import re

import yaml
from conda.plan import execute_plan

from .cpp_flatten import cpp_flatten
from .estimate_tokens import num_tokens_from_string
from .get_coverage import get_coverage_python
from .get_coverage_cc import get_coverage_cc
from .implied_graph import get_implied_graph_cc
from .implied_graph import get_implied_graph_python
from .symbolic import get_symbolic_test, parse_cex, has_symbolic_failed, get_extern_interface
from .interfaces import language_interfaces, is_c_cxx, is_python, is_c, is_cxx
from .model import GetModel
from .prompts_anthropic import *
#from .prompts import *
from .utils import *

DEBUG = int(os.getenv('DEBUG', 0))

def get_extern_definition(text):
    '''
    Extracts dictionary from the following structure generated by the LLM.

    <plan-of-action>
    List[str]
    </plan-of-action>
    Wait
    <extern>
    List[str]
    </extern>

    :param text: text to be scanned.
    :return: extern definition.
    '''

    text_list = text.split('\n')
    s_extern = '<extern>'
    e_extern = '</extern>'
    for l in range(len(text_list)):
        if s_extern in text_list[l].strip():
            l += 1
            break
    extern = []
    for l in range(l, len(text_list)):
        if e_extern in text_list[l].strip():
            break
        extern.append(text_list[l])

    return '\n'.join(extern)

def remove_backticks(text):
    text_list = text.split('\n')
    if text_list[0][:3] == "```":
        text_list = text_list[1:]
    if text_list[-1][:3] == "```":
        text_list = text_list[:-1]
    return "\n".join(text_list)

def get_unit_test(text):
    '''
    Extracts dictionary from the following structure generated by the LLM.

    <plan-of-action>
    List[str]
    </plan-of-action>
    Wait
    <test-code>
    List[str]
    </test-code>
    <explanation>
    List[str]
    </explanation>

    :param text:
    :return:
    '''
    text_list = text.split('\n')
    s_plan_of_action = '<plan-of-action>'
    e_plan_of_action = '</plan-of-action>'
    for l in range(len(text_list)):
        if s_plan_of_action in text_list[l].strip():
            l += 1
            break
    plan_of_action = []
    for l in range(l, len(text_list)):
        if e_plan_of_action in text_list[l].strip():
            break
        plan_of_action.append(text_list[l])

    s_test_code = '<test-code>'
    e_test_code = '</test-code>'
    for l in range(l, len(text_list)):
        if s_test_code in text_list[l].strip():
            l += 1
            if "```" in text_list[l]:
                l += 1
            break
    test_code = []
    for l in range(l, len(text_list)):
        if "```" in text_list[l] or e_test_code in text_list[l].strip():
            l += 1
            break
        test_code.append(text_list[l])

    s_explanation = '<explanation>'
    e_explanation = '</explanation>'
    for l in range(l, len(text_list)):
        if s_explanation in text_list[l].strip():
            l += 1
            break
    explanation = []
    for l in range(l, len(text_list)):
        if e_explanation in text_list[l].strip():
            break
        explanation.append(text_list[l])

    test_code = '\n'.join(test_code)

    test_code = re.sub(r'\&lt;', '<', test_code)
    test_code = re.sub(r'\&gt;', '>', test_code)
    return {
        "plan of action": plan_of_action,
        "test program": test_code,
        "explanation": explanation
    }


def extract_core(text, is_json=False):

    '''
        Extracts code from LLM output that is between ```.

        :param text: output from LLM.
        :param is_json: if True, consider the result a json file.

        :return: extracted code.
    '''

    if bool(os.getenv("UNITTENX_DEBUG_OUTPUT", "")):
        model_name = os.getenv('MODEL_NAME', 'openai')
        path = f'/tmp/unittenx/{model_name}'
        os.makedirs(path, exist_ok=True)
        file_id = 0
        while os.path.exists(f"{path}/output.p{file_id}"):
            file_id += 1
        with open(f"{path}/output.p{file_id}", "w") as f:
            f.write(text)

    '''try:
        le = text.index('```')
        text = text[le:]

        le = text.index('\n')
        text = text[le+1:]
    except:
        pass
    
    try:
        ri = text.index('```')
        text = text[:ri]
    except:
        pass'''

    if is_json:
        le = text.find('{')
        if le != -1:
            text = text[le:]

        ri = text.rfind('}')
        if ri != -1:
            text = text[:ri + 1]

    return text


# Define the function that determines whether to continue or not
def should_continue(state):
    '''
        Conditional branch to detect if another reflection lo

        :param state: state of the agent (we look at number_of_iterations).

        :return: 'continue' or 'end'.
    '''

    coverage_log = state.get("coverage_output", "")
    number_of_iterations = state["number_of_iterations"]
    max_number_of_iterations = state["max_number_of_iterations"]
    if (
        number_of_iterations > max_number_of_iterations and
        "FIX THIS" not in coverage_log and
        "core dumped" not in coverage_log
    ):
        return "end"
    # Otherwise if there is, we continue
    else:
        return "continue"


def has_symbolic_test(state):
    '''
        Conditional branch to if symbolic test generated a test case.
        If yes, do coverage + reflection before going back to unit-test.

        :param state: state of the agent (we look at number_of_iterations).

        :return: 'continue' or 'end'.
    '''

    unit_test = state.get("test", "")

    if unit_test:
        # if there is a unit-test, go to coverage + reflection.
        return "continue"
    else:
        # otherwise, go to unit-test to generate a test.
        return "skip"


def get_language_interface(language, source_files):
    '''
    If language == 'auto' detect by extension, otherwise look at
    language_interface.

    :param language: one of language_interface (above) keys + 'auto'.
    :param source_files: source files if we want to detect automatically.

    :return: language and test environment to use.
    '''
    language = language.lower()
    if language == "auto":
        extensions = set([os.path.splitext(fn)[1][1:].lower() for fn in source_files])
        extensions = set([s if s not in ["c++", "cpp", "cxx", "cc"] else "cc"
                      for s in extensions])
        assert len(extensions) == 1
        language = list(extensions)[0]
        if is_python(language):
            language = "python"
    elif not (is_c_cxx(language) or is_python(language)):
        raise ValueError

    return language, language_interfaces[language]


# Define the function that extracts files and depth
def implied_functions(state, config):
    '''
        Computes functions in the transitive closure of code to a given depth.

        :param state: state of the agent.
        :param config: configuration of agent.

        :return: new state.
    '''
    print('=' * 80)
    print()
    print('entering detection of implied functions')

    depth = state["depth"]
    cflags = state.get("cflags", "")
    name = state["name"]
    project = state["project"]
    source_files = state["source_files"]
    includes = state.get("includes", [])
    extern_interface = {
        "interface": "",
        "files_to_include": []
    }

    try:
        language, test_interface = get_language_interface(
            state["language"], source_files)
    except:
        language = "not supported"
        test_interface = "none"

    if is_c(language):
        test_language = "C++"
    else:
        test_language = language

    declaration_lines = []

    model_name = config.get('configurable', {}).get("model_name", "gpt-4o")
    model = GetModel(model_name)

    if True:
        function_names = None
        if is_c_cxx(language):
            function_names = get_implied_graph_cc(
                project, source_files, function=name, depth=depth, cflags=cflags)
            with open(os.path.join(project, "db.yaml"), "r") as f:
                project_yaml = yaml.safe_load(f)
            # get project info
            basename, _ = os.path.splitext(os.path.basename(source_files[0]))
            with open(os.path.join(project, 'info', basename + '.info'), "r") as f:
                project_name = yaml.safe_load(f)[-1]["name"]
            interface, declaration_lines, files_to_include = get_extern_interface(
                project_yaml['files'][project_name], source_files[0], name, cflags, includes)
            extern_interface = {
                "interface": interface,
                "files_to_include": files_to_include
            }

        elif is_python(language):
            function_names = get_implied_graph_python(
                project, source_files, function=name, depth=depth)
    else:
        # if something happens use everything
        function_names = None
    # there is nothing to search, there was either an error
    # or invalid language, we need to stop
    if not function_names:
        raise ValueError(
            f"Could not detect {name} in {','.join(source_files)} or we got " +
            "a compilation error.")

    # now read the files
    source_code = []
    for filename in source_files:
        try:
            if is_python(language):
                source = open(filename, 'r').read()
            elif is_c_cxx(language):
                try:
                    source = cpp_flatten(filename, cflags, includes)
                    if extern_interface["interface"]:
                        messages = [
                            {
                                'role': 'user',
                                'content': implied_functions_prompt.format(
                                    source_code=source,
                                    structures=extern_interface["interface"]
                                )
                            }]

                        if DEBUG:
                            print('-' * 80)
                            print(messages[0]['content'])
                            if DEBUG >= 2: input('<implied-functions-prompt> continue:')
                        response = model.invoke(messages)
                        output = extract_core(response.content)

                        print('\n')
                        print(output)

                        output = get_extern_definition(output)

                        if DEBUG:
                            if DEBUG >= 2: input('<implied-functions-result> continue:')

                        extern_interface["interface"] = output
                        extern_interface["files_to_include"] = []

                except:
                    # this should not really be correct, but in case everything
                    # else fails...
                    source = open(filename, 'r').read()
                source = '\n'.join([
                    s.rstrip() for s in source.split('\n') if s.strip()])
            else:
                raise ValueError
        except:
            source = f"no source code found for {filename}."

        if isinstance(function_names, type(None)):
            # just pick one
            if name in source:
                function_names = { name: filename }
        source_code.append(
            f'- file: {filename}\n```{language}\n{source}\n```\n')
    return {
        'source_code': '\n'.join(source_code),
        'source_files': source_files,
        'language': language,
        'test_language': test_language,
        'test_interface': test_interface,
        'function_names': function_names,
        'extern_interface': extern_interface,
        'declaration_lines': declaration_lines
    }


# Define the function that generates coverage using symbolic
def symbolic(state, config):
    '''
        Generates unit tests using symbolic engine.

        THIS IS A PLACEHOLDER RIGHT NOW.

        :param state: state of the agent.
        :param config: configuration of agent.

        :return: new state.
    '''

    language = state["language"]
    source_code = state["source_code"]
    source_files = state["source_files"]
    cflags = state.get("cflags", "")
    target_type = state["target_type"]
    name = state["name"]
    project = state["project"]
    includes = state.get("includes", [])
    work = state["work"]
    test_interface = state["test_interface"]
    function_names = state["function_names"]

    print('=' * 80)
    print()
    print('entering symbolic engine')

    if is_c(language):

        # we use the source code that is defined in function_names,
        # or the first filename in source_files.
        filename = function_names.get(name, source_files[0])

        print()

        cex_list, logs = get_symbolic_test(
            filename=filename,
            cflags=cflags, 
            target=name,
            project=project,
            work=work
        )

        logs = "\n\n".join(logs)

        # something happened, and we could not get the symbolic
        # engine to return something useful. we will proceed
        # without symbolic results.
        got_an_error = has_symbolic_failed(logs)

        if not cex_list or got_an_error:
            print("  ... could not generate symbolic testcases.")
            print(logs)
            return {'test': '', 'symbolic_test_cases': ''}

        test_cases = parse_cex(cex_list).replace("'", '')

        print(test_cases)

        messages = [
            { 
                'role': 'user', 
                'content': symbolic_test_prompt.format(
                    target_type=target_type,
                    name=name,
                    test_language="C++",
                    test_interface=test_interface,
                    source_code=source_code,
                    test_cases=test_cases
                )
            }]

        num_tokens = num_tokens_from_string(messages[-1]['content'])
        print(f'... number of tokens: {num_tokens}')

        model_name = config.get('configurable', {}).get("model_name", "gpt-4o")
        model = GetModel(model_name)
        if DEBUG:
            print('-' * 80)
            print(messages[0]['content'])
            if DEBUG >= 2: input('<symbolic-prompt> continue:')
        response = model.invoke(messages)
        output = remove_backticks(response.content)
        if DEBUG:
            print('-' * 80)
            print(output)
            if DEBUG >= 2: input('<symbolic-result> continue:')

        messages.append(response)

    else:
        output = ""
        messages = []
        test_cases = ""
        print(f'    symbolic engine not available to {language}')

    return { 'test': output, "messages": messages, "symbolic_test_cases": test_cases }


# Define the function that generates unit test
def unit_test(state, config):
    '''
        Computes the unit test of the function, according to prompt.

        :param state: state of the agent.
        :param config: configuration of agent.

        :return: new state.
    '''
    number_of_iterations = state["number_of_iterations"]
    print()
    print('-' * 80)
    print(f"\nDoing iteration {number_of_iterations}\n\n")

    print('=' * 80)
    print()
    print('entering unit-test generation')

    source_code = state["source_code"]
    source_files = state["source_files"]
    language = state["language"]
    test_language = state["test_language"]
    test_interface = state["test_interface"]
    reviews = state["review"]
    target_type = state["target_type"]
    name = state["name"]
    function_names = state["function_names"]
    missing_coverage = state["missing_coverage"]
    coverage_output = state["coverage_output"]
    messages = state["messages"]
    unit_test = state.get("test", "")
    with_messages = state["with_messages"]
    symbolic_test_cases = state["symbolic_test_cases"]
    extern_interface = state["extern_interface"]

    if reviews:
        reviews = yaml.safe_dump(list(reversed(reviews)))
    else:
        reviews = ''

    if is_python(language):
        test_packages = []
        for source in source_files:
            test_packages.append(
                'from ' +
                os.path.splitext(os.path.basename(source))[0] +
                ' import *'
            )
        additional_requirements = (
            "- Your test must look like the following:\n"
            "\n"
            "```python\n"
            "def test_case_<description>():\n"
            "    <set values to the inputs>\n"
            f"    {name}(values)\n"
            "    <check if values are expected>\n"
            "```\n"
            "\n"
            "- Unless you want to test missing parameters, you MUST provide the correct number of parameters to the functions.\n"
            "\n"
            "- Your test MUST include the following import statements:\n"
            "\n"
            '```python\n')
        additional_requirements += '\n'.join(test_packages) + '\n'
        additional_requirements += (
            "```\n"
            "\n"
            "- Your test MUST consider normal execution MUST NOT to expect exceptions.\n"
        )
    elif is_c_cxx(language):
        assert_and_dup = (
            "- You MUST fix compilation errors from your test before adding anything else. \n"
            "    -- The narrowing error in `char q[2] = {0, static_cast<unsigned char>(255)};` can be  fixed to `char qtype[2] = {0, char(255)};`\n"
            "    -- The narrowing error in `char q[2] = {0, 255};` can be  fixed to `char qtype[2] = {0, char(255)};`\n"
            "\n"
            "- Strings MUST NOT mix hexadecimal values and characters in the same assignment. You MUST use ONLY one of them.\n"
            "\n"
            "- Assertions with your code MUST first print what was the output expected output before raising a assertion failure. For example, instead of doing `assert (expected == actual);`, you MUST do as in following code snippet for the global variable `number_of_failures`:\n"
            "\n"
            "```cpp\n"
            "if (expected != actual) {\n"
            "    cout << \"Expected: \" << expected << endl << \"Actual: \" << actual << endl << flush; number_of_failures++; "
            "    throw std::runtime_error(\"error\");\n"
            "}\n"
            "```\n"
            "\n"
            "- If your unit test uses std packages, you MUST add `using namespace std;`.\n"
            "\n"
        )
        if 'plain' in test_interface:
            additional_requirements = (
                "- Your test program MUST work like pytest interface. It must be written in C++ and use try/catch mechanism to capture errors, so that test programs do not crash.\n"
                "\n"
                "- You MUST also count the number of failures so that you report that at the end, just like what a test in pytest do.\n"
                "\n"
                "- You MUST always flush all outputs with flush as in the example: `cout << \"Hello, world!\" << endl << flush;`\n"
                "\n"
            )
        else:
            additional_requirements = ""
        interface = (
                [ f'    #include "{file}"' for file in extern_interface["files_to_include"] ]
                + [ f'    {extern_interface["interface"]};']
        )
        additional_requirements += (
            "- Function execution is guarded by the following timeout guard function:\n"
            "       void run_with_timeout(std::function<void()> func) {\n"
            "           const char* timeout_env = std::getenv(\"UNITTENX_TIMEOUT\");\n"
            "           int timeout_seconds;\n"
            "           if (timeout_env == nullptr) timeout_seconds = 1;\n"
            "           else timeout_seconds = std::stoi(timeout_env);\n"
            "           std::packaged_task<void()> task(func);\n"
            "           std::future<void> future = task.get_future();\n"
            "           std::thread(std::move(task)).detach();\n"
            "           std::chrono::seconds timeout(timeout_seconds);\n"
            "           if (future.wait_for(timeout) == std::future_status::timeout) {\n"
            "               cout << \"ERROR: function executed exceeded timeout limit.\" << endl << flush;\n"
            "               throw std::runtime_error(\"error\");\n"
            "           }\n"
            "           \n"
            "           // Check if the future holds an exception and rethrow it\n"
            "           try {\n"
            "               future.get(); // Will rethrow the exception if func threw one\n"
            "           } catch (...) {\n"
            "               // Optionally, log the exception or take other actions\n"
            "               throw; // Rethrow the caught exception\n"
            "           }\n"
            "       }\n"
            f"- Your test code to test `{name}` should be written as in this example:\n"
            "\n"
            "    ```cpp\n"
            "       void test_case_1() {\n"
            "           cout << \"Starting test case 1\" << endl;\n"
            "           <initialization of test case 1>\n"
            "           <assignment of parameters and expected values>\n"
            f"          {name}(<parameters>);\n"
            "           cout << \"Test case 1 completed\" << endl;\n"
            "       }\n"
            "       ...\n"
            "       void test_case_2() {\n"
            "           ...\n"
            f"           {name}(value);\n"
            "           ...\n"
            "       }\n"
            "       ...\n"
            "        void main() {\n"
            "            int number_of_failures=0;\n"
            "            try {\n"
            "                cout << \"Test case 1: <description of test case 1>\" << endl;\n"
            "                run_with_timeout(test_case_1);\n"
            "            } catch (...) {\n"
            "                number_of_failures++;\n"
            "                cout << \"Exception in test case: <description of test case 1>\" << endl << flush;\n"
            "            }\n"
            "            try {\n"
            "                cout << \"Test case 2: <description of test case 2>\" << endl;\n"
            "                run_with_timeout(test_case_2);\n"
            "            } catch (...) {\n"
            "                number_of_failures++;\n"
            "                cout << \"Exception in test case: <description of test case 2>\" << endl << flush;\n"
            "            }\n"
            "            ...\n"
            "            cout << \"Number of failures: \" << number_of_failures << endl << flush;\n"
            "            return 0;\n"
            "        }\n"
            "```\n"
            "If `test_case_1` crashes during execution, you will change the code in the main function to comment out the invocation of test_case_1, as in the following code snippet:\n"
            "\n"
            "    ```cpp\n"
            "       void test_case_1() {\n"
            "           cout << \"Use case: Test case 1: case <text>\" << endl;\n"
            "           try {\n"
            f"               {name}(value);\n"
            "           } catch (...) {\n"
            "               cout << \"Exception occurred in Test case 1\" << endl;\n"
            "           }\n"
            "           cout << \"Test case 1 completed\" << endl;\n"
            "       }\n"
            "       ...\n"
            "       void test_case_2() {\n"
            "           ...\n"
            f"           {name}(value);\n"
            "           ...\n"
            "       }\n"
            "       ...\n"
            "        void main() {\n"
            "            // CRASH: Test case 1 is commented out because it is crashing.\n"
            "            // try {\n"
            "            //     run_with_timeout(test_case_1);\n"
            "            // } catch (...) {\n"
            "            //     number_of_failures++;\n"
            "            //     cout << \"Exception in test case: <description of test case 1>\" << endl << flush;\n"
            "            // }\n"
            "            try {\n"
            "                run_with_timeout(test_case_2);\n"
            "            } catch (...) {\n"
            "                number_of_failures++;\n"
            "                cout << \"Exception in test case: <description of test case 2>\" << endl << flush;\n"
            "            }\n"
            "            ...\n"
            "            cout << \"Number of failures: \" << number_of_failures << endl << flush;\n"
            "            return 0;\n"
            "        }\n"
            "```\n"
            "\n"
            "Note: The code inside each test case function (e.g., test_case_1()) MUST remain unchanged. Only the invocation of the test case function in the main function should be commented out if it crashes. The code in each test case should always set the initial values for parameters, run the function, and check the results without any modifications.\n"
            "\n"
            "- Always include in the headers `#include <climits>` and `#include <future>` to your tests.\n"
            "\n"
            "- If your unit test calls a function from the source code, you MUST write the function as an extern function with the correct definition.\n"
            "\n"
            "- If accessing extern 'C' functions from 'C++' programs, you must use 'extern \"C\" { `C include files` and declarations }' from the syntax with the correct function interfaces if the include files are present in the source code.\n"
            f"For example, you MUST have the following extern interface for {name} and nothing else:\n"
            f"'extern \"C\" {{\n{'\n'.join(interface)}\n}}'"
            "\n"
            "- Before executing each test, you must output in stdout what's the test is about, and that the test has completed, just like how PyTest works. For example, if you will execute the test function `test_use_case(<value>, <input>, <output>);`, you must first output `cout << \"Use case <description\" << endl;` before executing test.\n"
            "\n"
            "- Function `main` should always return 0."
            "\n"
        )

        if with_messages:
            additional_requirements += (
                "- Before you redirect any input and output, you MUST do as in the following code snippet:\n"
                "\n"
                "```cpp\n"
                "  cout << \"Testing use case\" << value << endl\n"
                "  <redirecting stdin and stdout>\n"
                "  test_use_case(<value>, <input>, <output>);\n"
                "  <restoring redirection of stdin and stdout>\n"
                "```\n"
                "\n"
                "- Your test must be coded such that it MUST NEVER use the 'Testing use case' message as the expected output or actual output.\n"
                "\n"
            )

        additional_requirements += (
            f"{assert_and_dup}"
            "- You SHOULD NOT include the source files, as they will be compiled separately in a Makefile.\n"
        )

    if with_messages:
        with_messages = (
            "- Besides checking the code coverage, valid values and types, critical values, return values, and exception errors, you MUST also capture any input and output messages.\n"
        )
    else:
        with_messages = (
            "- Your unit-test MUST ONLY check for code coverage, valid values and types, critical values, return values, and exception errors,\n"
            "\n"
            "- You SHOULD NEVER test input and output messages.\n"
        )

    #messages = messages + [ 
    messages = [ 
            { 
                'role': 'user', 
                'content': unit_test_prompt.format(
                    target_type=target_type,
                    name=name,
                    test_language=test_language,
                    language=language,
                    test_interface=test_interface,
                    source_code=source_code,
                    missing_coverage=missing_coverage,
                    coverage_output=coverage_output,
                    additional_requirements=additional_requirements,
                    with_messages=with_messages,
                    unit_test=unit_test,
                    function_names=yaml.safe_dump(list(function_names.keys())),
                    review=reviews,
                    test_cases=symbolic_test_cases)
            }]

    # print(messages[-1]['content'])
    # input('wait:')

    num_tokens = num_tokens_from_string(messages[-1]['content'])
    print()
    print(f'... number of tokens: {num_tokens}')

    #print(messages[-1]['content'])
    #input('wait:')

    model_name = config.get('configurable', {}).get("model_name", "gpt-4o")
    model = GetModel(model_name)
    if DEBUG:
        print('-' * 80)
        print(messages[0]['content'])
        if DEBUG >= 2: input('<unit-test-prompt> continue:')
    response = model.invoke(messages)

    js = get_unit_test(response.content)

    plan_of_action = js["plan of action"]

    output = js["test program"]
    explanation = js["explanation"]
    print()
    print("Plan of Action:")
    print()
    print(yaml.safe_dump(plan_of_action))

    #print()
    #print("Test code:")
    #print()
    #print(output)
    #input('<unit-test-result> continue:')

    # output = extract_core(response.content)
    if DEBUG:
        print('unit_test: ' + '-' * 74)
        print(output)
        print('-' * 80)
        if DEBUG >= 2: input('<unit-test-result> continue:')

    return { "test": output, "messages": messages + [response] }


# Define the function that calls the model
def reflection(state, config):
    '''
        Reviews the test case generated.

        :param state: state of the agent.
        :param config: configuration of agent.

        :return: new state.
    '''
    print('=' * 80)
    print()
    print('entering reflection review of unit-test')
    print()

    number_of_iterations = state["number_of_iterations"]
    target_type = state["target_type"]
    name = state["name"]
    source_code = state["source_code"]
    coverage_output = state["coverage_output"]
    unit_test = state["test"]
    reviews = state["review"]
    function_names = state["function_names"]
    messages = state["messages"]
    test_language = state["test_language"]

    if reviews:
        reviews = yaml.safe_dump(list(reversed(reviews)))
    else:
        reviews = ''

    #messages = messages + [
    messages = [
            { 
                'role': 'user', 
                'content': reflection_prompt.format(
                    test_language=test_language,
                    target_type=target_type,
                    name=name,
                    source_code=source_code,
                    coverage_output=coverage_output,
                    function_names=yaml.safe_dump(list(function_names.keys())),
                    unit_test=unit_test,
                    reviews=reviews)
            }]

    num_tokens = num_tokens_from_string(messages[-1]['content'])
    print(f'... number of tokens: {num_tokens}')
    print()

    if DEBUG:
        print('-' * 80)
        print(messages[-1]['content'])
        if DEBUG >= 2: input('<reflection-prompt> continue:')
    model_name = config.get('configurable', {}).get("model_name", "gpt-4o")
    model = GetModel(model_name)
    response = model.invoke(messages)
    # OpenAI extracts the JSON file, other models do not.

    output = extract_core(response.content, is_json=True)
    try:
        js = json.loads(output)
    except:
        fatal_error(f'Reflection with output\n{output}')
        # import pdb; pdb.set_trace()

    if DEBUG:
        print('-' * 80)
        print(output)
        if DEBUG >= 2: input('<reflection-result> continue:')

    try:
        rating = js["rating"]
    except:
        fatal_error(f'Reflection cannot find "rating"\n{yaml.safe_dump(js)}')
        # import pdb; pdb.set_trace()

    print(yaml.safe_dump(js))

    reviews = state["review"] + [js]

    return {
            "number_of_iterations": number_of_iterations + 1, 
            "review": reviews,
            "messages": messages + [response]
           }


# Define the function that calls the model
def coverage(state, config):
    '''
        Gets coverage information.

        :param state: state of the agent.
        :param config: configuration of agent.

        :return: new state.
    '''
    print('=' * 80)
    print()
    print('entering coverage hole extraction')
    print()

    test = state["test"]
    # reviews = state["review"]
    name = state["name"]
    src_dir = state["project"]
    function_names = state["function_names"]
    source_files = state["source_files"]
    language = state["language"]
    cflags = state.get("cflags", "")
    ldflags = state.get("ldflags", "")
    work = state["work"]
    ssh = state.get("ssh", "")
    max_number_of_iterations = state["max_number_of_iterations"]
    declaration_lines = state["declaration_lines"]

    if is_python(language):
        test_dir = work
        if True:
            with open(f'{test_dir}/test_{name}.py','w') as fp:
                fp.write(test)

            fix_this, missing_coverage, coverage_log = get_coverage_python(
                test_dir=test_dir,
                src_dir=src_dir,
                files=[os.path.basename(s) for s in source_files],
                functions=function_names,
                ssh=ssh,
                debug=DEBUG,
            )
            # Generate Makefile
            paths = []
            for source in source_files:
                p, _ = os.path.split(source)
                paths.append(p)
            paths = ':'.join(list(set(paths)))
            with open(f'{work}/Makefile', 'w') as fp:
                fp.write(
                     'DOLLAR = $\n'
                     'run:\n'
                     f'\tcoverage erase \n'
                     f'\tPYTHONPATH="$(DOLLAR)PYTHONPATH:{paths}" coverage run -m pytest \n'
                     '\tcoverage json\n')
    elif is_c_cxx(language):
        test_dir = work
        # test is written in C++ to use try/catch mechanisms to avoid crashes.
        if True: # with tempfile.TemporaryDirectory() as test_dir:
            with open(f'{test_dir}/test_{name}.cc','w') as fp:
                fp.write(test)

            fix_this, missing_coverage, coverage_log = get_coverage_cc(
                test_dir=test_dir,
                src_dir=src_dir,
                files=source_files,
                functions=function_names,
                cflags=cflags,
                ldflags=ldflags,
                ssh=ssh,
                declaration_lines=declaration_lines,
                debug=DEBUG,
            )

            # reread test as get_coverage_cc may have changed it commenting out crashes.
            with open(f'{test_dir}/test_{name}.cc','r') as fp:
                test = fp.read()

    if fix_this:
        if 'core dump' in coverage_log:
            message = 'YOU MUST FIX THIS CORE DUMP TO PROCEED AT LEAST COMMENTING OUT THE FAILING TEST.'
        else:
            message = 'YOU MUST FIX THIS ERROR FROM THIS LOG'
        coverage_log = message + ':\n\n' + coverage_log
        # if there is a critical test error that does not enable us to computer coverage,
        # disregard this iteration by increasing the number of maximum iterations.
        print("... disregarding this iteration as it failed compilation or caused core dump.\n")
        max_number_of_iterations += 1

    if missing_coverage:
        for line in missing_coverage:
            print(f"    {line}")
        print()
        if DEBUG >= 2: input('<coverage-result> continue:')
        missing_coverage = yaml.safe_dump(missing_coverage)
    else:
        print("    no missing coverage found.\n")
        missing_coverage = ''

    return {
            "test": test,
            "coverage_output": coverage_log,
            "missing_coverage": missing_coverage,
            "max_number_of_iterations": max_number_of_iterations
           }
